{
    "collab_server" : "",
    "contents" : "library(MASS) # NB: this will mask dplyr::select\n\n### ORDERING DATA\nx <- paste('x', as.character(c(1:101)), sep='')\ny <- paste('y', as.character(c(1:101)), sep='')\n\n# Each x and y coordenate into two columns (101 coordenates per trial) \nnormalized_positions = calibration_data %>%\n  dplyr::select(Subject, Item.number, Polarity, Expected_response, Normalized.positions.X,Normalized.positions.Y) %>%\n  separate(Normalized.positions.Y, into= y, sep = \",\") %>%\n  separate(Normalized.positions.X, into= x, sep = \",\")  \nnormalized_positions[y] <- sapply(normalized_positions[y],as.numeric)\nnormalized_positions[x] <- sapply(normalized_positions[x],as.numeric)\nsapply(normalized_positions, class)\n\npar(mfrow=c(2,2))\nhist(normalized_positions$x10)\nhist(normalized_positions$x30)\nhist(normalized_positions$x60)\nhist(normalized_positions$x90)\n\n\n\n# Taking the negative of false items, to have everything in the same scale\nnormalized_positions_false = normalized_positions%>%\n  filter(Expected_response=='false')%>%\n  dplyr::mutate_at(vars(starts_with('x')), funs('-'))\nnormalized_positions_true = filter(normalized_positions, Expected_response=='true')\nnormalized_positions = bind_rows(normalized_positions_false,normalized_positions_true)\nrm(normalized_positions_true, normalized_positions_false)\n\n#last arrangements\nnormalized_positions$Subject <- factor(normalized_positions$Subject)\nnormalized_positions$Polarity <- factor(normalized_positions$Polarity)\nnormalized_positions$Expected_response <- factor(normalized_positions$Expected_response)\n\n#Correlation matrix\nx_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"x\"))) > 0.95\ny_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"y\"))) > 0.95\n\n# Remove points after 2 correlation > .99 (otherwise I'll be removing too many points)\nx_correlations.sums <- colSums(x_correlations)\ny_correlations.sums <- colSums(y_correlations)\n\n##Remove points after 2 correlation > .99 (otherwise I'll be removing too many points)\n# for(c in 3:101) {\n#   for (r in 1:101) {\n#     m <- paste('x', as.character(c), sep='')\n#     l <- paste('x', as.character(c-1), sep='')\n#     if(x_correlations[r,m] == TRUE & x_correlations[r,l] == TRUE) \n#     {x[r]='SACAR'}\n#   }}\n# for(c in 3:101) {\n#   for (r in 1:101) {\n#     m <- paste('y', as.character(c), sep='')\n#     l <- paste('y', as.character(c-1), sep='')\n#     if(y_correlations[r,m] == TRUE & y_correlations[r,l] == TRUE) \n#     {y[r]='SACAR'}\n#   }}\n\n\n######\n##Remove points after 3 correlation > .99 (otherwise I'll be removing too many points)\n\nfor (r in 1:100){\n  if (x[r]!='SACAR'){\n  for (c in (r+1):101){\n    m <- paste('x', as.character(c), sep='')\n    if(x_correlations[r,m] == TRUE)\n    {x[c]='SACAR'}\n    else{break}\n    }}}\n\n\nfor (r in 1:100){\n  if (y[r]!='SACAR'){\n    for (c in (r+1):101){\n      m <- paste('y', as.character(c), sep='')\n      if(y_correlations[r,m] == TRUE)\n      {y[c]='SACAR'}\n      else{break}\n    }}}\n\n\n\n#Taking out elements\nx.subset <- x[x != \"SACAR\"];\ny.subset <- y[y != \"SACAR\"];\n\n\n### LDA\nnormalized_positions.new <- normalized_positions %>%\ndplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset), one_of(y.subset))%>%\n#  dplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset))%>%  \n  mutate(Deviation=ifelse(Polarity == \"deviated\", \"NonCentral\", \"Central\"))\n\njpeg('histograms_positions(after exclusion).jpg')\npar(mfrow=c(2,2))\nhist(normalized_positions.new$x13)\nhist(normalized_positions.new$x35)\nhist(normalized_positions.new$x68)\nhist(normalized_positions.new$x95)\ndev.off()\n\nm_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\")),\n#m_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\")),\n             grouping=normalized_positions.new$Deviation)\n\nv_lda <- m_lda$scaling\nb_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda)\n#b_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda)\nsave(v_lda, b_lda, x.subset, y.subset, file=\"transformation_all.RData\")\n\n#Creating matrix with the lda meaur\nlda_measure.df <- data_frame(\n  lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda- b_lda),\n  #lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda- b_lda),\n  Deviation=normalized_positions.new$Deviation, \n  Subject = normalized_positions.new$Subject, \n  Expected_response = normalized_positions.new$Expected_response,\n  Item.number = normalized_positions.new$Item.number)\n \nggplot(lda_measure.df, aes(x=lda_measure, fill=Deviation)) + \n  geom_histogram(binwidth=.5,  position=\"dodge\")+ \n  theme(legend.position = \"top\") + \n  facet_grid(.~Expected_response)\nggsave('LDA-classification_all.png', plot = last_plot(), scale = 1, dpi = 300)\n\n###SAVING THIS DATA\ncalibration_data <- dplyr::full_join(lda_measure.df, calibration_data, by=c(\"Subject\", \"Item.number\", \"Expected_response\"))\n\n\n#Last tests\nmymatrix <- as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\")))\ntest <- t(t(mymatrix)  * as.vector(v_lda)) # first optono\n\ndev.off()\nhist(test.df)\n\n\nggplot(data = melt(as.data.frame(test)), mapping = aes(x = value)) + ggtitle('calibration')+\n  geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')+ theme(strip.text = element_text(size=7), axis.text=element_text(size=7))\n\n\nggplot(data = melt(as.data.frame(mymatrix)), mapping = aes(x = value)) + ggtitle('calibration')+\n  geom_histogram(bins = 20) + facet_wrap(~variable)+ theme(strip.text = element_text(size=7), axis.text=element_text(size=7))\n\n\n\n\nggplot(data = dplyr::select(normalized_positions.new, y96, Deviation), aes(x=y96, fill=Deviation))+geom_histogram()\n\nggplot(data = dplyr::select(normalized_positions.new, y99, Deviation), aes(x=y99, fill=Deviation))+geom_histogram()\n\n\n\n",
    "created" : 1492022581563.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2552998369",
    "id" : "1EE72626",
    "lastKnownWriteTime" : 1491986010,
    "last_content_update" : 1491986010,
    "path" : "~/WebstormProjects/negationMT/R_scripts/LDA.R",
    "project_path" : "R_scripts/LDA.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}