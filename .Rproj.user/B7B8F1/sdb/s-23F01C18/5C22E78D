{
    "collab_server" : "",
    "contents" : "library(MASS) # NB: this will mask dplyr::select\n\n### ORDERING DATA\nx <- paste('x', as.character(c(1:101)), sep='')\ny <- paste('y', as.character(c(1:101)), sep='')\n\n# Each x and y coordenate into two columns (101 coordenates per trial) \nnormalized_positions = calibration_data %>%\n  dplyr::select(Subject, Item.number, Polarity, Expected_response, Normalized.positions.X,Normalized.positions.Y) %>%\n  separate(Normalized.positions.Y, into= y, sep = \",\") %>%\n  separate(Normalized.positions.X, into= x, sep = \",\")  \nnormalized_positions[y] <- sapply(normalized_positions[y],as.numeric)\nnormalized_positions[x] <- sapply(normalized_positions[x],as.numeric)\nsapply(normalized_positions, class)\n\n# Taking the negative of false items, to have everything in the same scale\nnormalized_positions_false = normalized_positions%>%\n  filter(Expected_response=='false')%>%\n  dplyr::mutate_at(vars(starts_with('x')), funs('-'))\nnormalized_positions_true = filter(normalized_positions, Expected_response=='true')\nnormalized_positions = bind_rows(normalized_positions_false,normalized_positions_true)\nrm(normalized_positions_true, normalized_positions_false)\n\n#last arrangements\nnormalized_positions$Subject <- factor(normalized_positions$Subject)\nnormalized_positions$Polarity <- factor(normalized_positions$Polarity)\nnormalized_positions$Expected_response <- factor(normalized_positions$Expected_response)\n\n\n#Taking the derivatives (EVERYTHING is here but I commented the delta_delta just to see if it works better, spoiler alert: it doesn't)\n\n#first derivative delta\nnormalized_positions_x <- dplyr::select(normalized_positions, starts_with(\"x\"))\nnormalized_positions_x$x1_delta <-  normalized_positions_x$x1 - 0\nnormalized_positions_y <- dplyr::select(normalized_positions, starts_with(\"y\"))\nnormalized_positions_y$y1_delta <-  normalized_positions_y$y1 - 0\n#x\nfor(c in 2:101)\n  { \n  name <- paste(x[c],'_delta', sep='')  \n  normalized_positions_x[, ncol(normalized_positions_x) + 1] <-  normalized_positions_x[c] - normalized_positions_x[c-1]\n  names(normalized_positions_x)[ncol(normalized_positions_x)] <- name\n  }\n\n#y\nfor(c in 2:101)\n{ \n  name <- paste(y[c],'_delta', sep='')  \n  normalized_positions_y[, ncol(normalized_positions_y) + 1] <-  normalized_positions_y[c] - normalized_positions_y[c-1]\n  names(normalized_positions_y)[ncol(normalized_positions_y)] <- name\n}\n\n\n# \n# # second derivative delta-delta\n# #x\n# normalized_positions_x$x1_delta_delta <-  normalized_positions_x$x1_delta - 0\n# for(c in 103:202)\n# {\n#   name <- paste(colnames(normalized_positions_x)[c],'_delta', sep='')  \n#   normalized_positions_x[, ncol(normalized_positions_x) + 1] <-  normalized_positions_x[c] - normalized_positions_x[c-1]\n#   names(normalized_positions_x)[ncol(normalized_positions_x)] <- name\n# }\n# \n# #y\n# normalized_positions_y$y1_delta_delta <-  normalized_positions_y$y1_delta - 0\n# for(c in 103:202)\n# {\n#   name <- paste(colnames(normalized_positions_y)[c],'_delta', sep='')  \n#   normalized_positions_y[, ncol(normalized_positions_y) + 1] <-  normalized_positions_y[c] - normalized_positions_y[c-1]\n#   names(normalized_positions_y)[ncol(normalized_positions_y)] <- name\n# }\n \n\n#this are all zeros so I remove them\nnormalized_positions_x$x101_delta <- NULL\n normalized_positions_y$y101_delta <- NULL\n normalized_positions_y$y100_delta <- NULL\n# normalized_positions_y$y101_delta_delta <- NULL\n# \n\nnormalized_positions <- merge(normalized_positions, normalized_positions_x)\nnormalized_positions <- merge(normalized_positions, normalized_positions_y)\n\n\n\n#Correlation matrix\nx_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"x\"))) > 0.95\ny_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"y\"))) > 0.95\n\n#Include all the values to remove correlated dimensions\nx <- c(x, paste(x, '_delta', sep=''))\ny <- c(y, paste(y, '_delta', sep=''))\n#x <- c(x, paste(x, '_delta', sep=''), paste(x, '_delta_delta', sep=''))\n#y <- c(y, paste(y, '_delta', sep=''), paste(y, '_delta_delta', sep=''))\n\n\n##Remove points after 3 correlation > .95 (otherwise I'll be removing too many points)\nfor (r in 1:200){\n  if (x[r]!='SACAR'){\n    for (c in (r+1):201){\n      m <- paste('x', as.character(c), sep='')\n      if(x_correlations[r,c] == TRUE)\n      {x[c]='SACAR'}\n      else{break}\n    }}}\n\n\nfor (r in 1:199){\n  if (y[r]!='SACAR'){\n    for (c in (r+1):200){\n      m <- paste('y', as.character(c), sep='')\n      if(y_correlations[r,c] == TRUE)\n      {y[c]='SACAR'}\n      else{break}\n    }}}\n\n\n\n#Taking out elements\nx.subset <- x[x != \"SACAR\"];\nx.subset <- x.subset[x.subset != \"x101_delta\"];\ny.subset <- y[y != \"SACAR\"];\ny.subset <- y.subset[y.subset != \"y101_delta\"];\ny.subset <- y.subset[y.subset != \"y100_delta\"];\n#y.subset <- y.subset[y.subset != \"y101_delta_delta\"];\n\n### LDA\nnormalized_positions.new <- normalized_positions %>%\n  dplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset), one_of(y.subset))%>%\n  #  dplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset))%>%  \n  mutate(Deviation=ifelse(Polarity == \"deviated\", \"NonCentral\", \"Central\"))\n\n\n#look at predictors that have near zero variance\nx = nearZeroVar(normalized_positions.new, saveMetrics = TRUE)\nx[x[,\"zeroVar\"] + x[,\"nzv\"] > 0, ] \nnormalized_positions.new <- normalized_positions.new%>%dplyr::select(-x2_delta,-x6_delta,-x7_delta,-x9_delta,-x97_delta,-x99_delta,-x100_delta,-y95_delta) \n\n###THIS IS FAILING!!!\nm_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\")),\n             #m_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\")),\n             grouping=normalized_positions.new$Deviation)\n\n\n\n\nv_lda <- m_lda$scaling\nb_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda)\n#b_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda)\n\n#save(v_lda, b_lda, x.subset, y.subset, file=\"transformation_all.RData\")\n\n#Creating matrix with the lda meaure\nlda_measure.df <- data_frame(\n  lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda- b_lda),\n  #lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda- b_lda),\n  Deviation=normalized_positions.new$Deviation, \n  Subject = normalized_positions.new$Subject, \n  Expected_response = normalized_positions.new$Expected_response,\n  Item.number = normalized_positions.new$Item.number)\n\nggplot(lda_measure.df, aes(x=lda_measure, fill=Deviation)) + \n  geom_histogram(binwidth=.5,  position=\"dodge\")+ \n  theme(legend.position = \"top\") + \n  facet_grid(.~Expected_response)",
    "created" : 1492011702846.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3214991862",
    "id" : "5C22E78D",
    "lastKnownWriteTime" : 1492022609,
    "last_content_update" : 1492022628150,
    "path" : "~/WebstormProjects/negationMT/LDA-Derivatives.R",
    "project_path" : "LDA-Derivatives.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}