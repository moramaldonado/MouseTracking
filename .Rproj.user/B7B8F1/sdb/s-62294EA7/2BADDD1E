{
    "collab_server" : "",
    "contents" : "library(MASS) # NB: this will mask dplyr::select\n\n### ORDERING DATA\nx <- paste('x', as.character(c(1:101)), sep='')\ny <- paste('y', as.character(c(1:101)), sep='')\n\n# Each x and y coordenate into two columns (101 coordenates per trial) \nnormalized_positions = calibration_data %>%\n  dplyr::select(Subject, Item.number, Polarity, Expected_response, Normalized.positions.X,Normalized.positions.Y) %>%\n  separate(Normalized.positions.Y, into= y, sep = \",\") %>%\n  separate(Normalized.positions.X, into= x, sep = \",\")  \nnormalized_positions[y] <- sapply(normalized_positions[y],as.numeric)\nnormalized_positions[x] <- sapply(normalized_positions[x],as.numeric)\nsapply(normalized_positions, class)\n\n\n# Taking the negative of false items, to have everything in the same scale\nnormalized_positions_false = normalized_positions%>%\n  filter(Expected_response=='false')%>%\n  dplyr::mutate_at(vars(starts_with('x')), funs('-'))\nnormalized_positions_true = filter(normalized_positions, Expected_response=='true')\nnormalized_positions = bind_rows(normalized_positions_false,normalized_positions_true)\nrm(normalized_positions_true, normalized_positions_false)\n\n#last arrangements\nnormalized_positions$Subject <- factor(normalized_positions$Subject)\nnormalized_positions$Item.number <- factor(normalized_positions$Item.number)\nnormalized_positions$Polarity <- factor(normalized_positions$Polarity)\nnormalized_positions$Expected_response <- factor(normalized_positions$Expected_response)\n\n#Correlation matrix\nx_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"x\"))) > 0.95\ny_correlations <- cor(dplyr::select(normalized_positions, starts_with(\"y\"))) > 0.95\n\n# Remove points after 2 correlation > .99 (otherwise I'll be removing too many points)\nx_correlations.sums <- colSums(x_correlations)\ny_correlations.sums <- colSums(y_correlations)\n\n##Remove points after 2 correlation > .99 (otherwise I'll be removing too many points)\n# for(c in 3:101) {\n#   for (r in 1:101) {\n#     m <- paste('x', as.character(c), sep='')\n#     l <- paste('x', as.character(c-1), sep='')\n#     if(x_correlations[r,m] == TRUE & x_correlations[r,l] == TRUE) \n#     {x[r]='SACAR'}\n#   }}\n# for(c in 3:101) {\n#   for (r in 1:101) {\n#     m <- paste('y', as.character(c), sep='')\n#     l <- paste('y', as.character(c-1), sep='')\n#     if(y_correlations[r,m] == TRUE & y_correlations[r,l] == TRUE) \n#     {y[r]='SACAR'}\n#   }}\n\n\n######\n##Remove points after 3 correlation > .99 (otherwise I'll be removing too many points)\n\nfor (r in 1:100){\n  if (x[r]!='SACAR'){\n  for (c in (r+1):101){\n    m <- paste('x', as.character(c), sep='')\n    if(x_correlations[r,m] == TRUE)\n    {x[c]='SACAR'}\n    else{break}\n    }}}\n\n\nfor (r in 1:100){\n  if (y[r]!='SACAR'){\n    for (c in (r+1):101){\n      m <- paste('y', as.character(c), sep='')\n      if(y_correlations[r,m] == TRUE)\n      {y[c]='SACAR'}\n      else{break}\n    }}}\n\n\n\n#Taking out elements\nx.subset <- x[x != \"SACAR\"];\ny.subset <- y[y != \"SACAR\"];\n\n\n### LDA\nnormalized_positions.new <- normalized_positions %>%\ndplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset), one_of(y.subset))%>%\n  #dplyr::select(Subject, Item.number, Polarity, Expected_response, one_of(x.subset))%>%  \n  mutate(Deviation=ifelse(Polarity == \"deviated\", \"NonCentral\", \"Central\"))\n\n\nm_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\")),\n#m_lda <- lda(x=dplyr::select(normalized_positions.new, starts_with(\"x\")),\n             grouping=normalized_positions.new$Deviation)\n\nv_lda <- m_lda$scaling\nb_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda)\n#b_lda <- mean(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda)\nsave(v_lda, b_lda, x.subset, y.subset, file=\"transformation.RData\")\n\n#Creating matrix with the lda meaur\nlda_measure.df <- data_frame(\n  lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"), starts_with(\"y\"))) %*% v_lda- b_lda),\n  #lda_measure=c(as.matrix(dplyr::select(normalized_positions.new, starts_with(\"x\"))) %*% v_lda- b_lda),\n\n  Deviation=c(normalized_positions.new$Deviation), \n  Subject = c(normalized_positions.new$Subject), \n  Expected_response = normalized_positions.new$Expected_response,\n  Item.number = c(normalized_positions.new$Item.number))\n \nggplot(lda_measure.df, aes(x=lda_measure, fill=Deviation)) + \n  geom_histogram(binwidth=.5,  position=\"dodge\")+ \n  theme(legend.position = \"top\") + \n  facet_grid(.~Expected_response)\nggsave('LDA-classification.png', plot = last_plot(), scale = 1, dpi = 300)\n\n###SAVING THIS DATA\ncalibration_data <- dplyr::full_join(lda_measure.df, calibration_data, by=c(\"Subject\", \"Item.number\", \"Expected_response\"))\n\n\n\n\n\n##Fake data (just in case)\n# normalized_positions.fake = calibration_data_new_subjects %>%\n#   dplyr::select(Subject, Polarity, Expected_response, Normalized.positions.X,Normalized.positions.Y) %>%\n#   separate(Normalized.positions.Y, into= y, sep = \",\", convert=T) %>%\n#   separate(Normalized.positions.X, into= x, sep = \",\", convert=T)\n# normalized_positions.fake$Subject <- factor(normalized_positions.fake$Subject)\n# normalized_positions.fake$Polarity <- factor(normalized_positions.fake$Polarity)\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1490706397272.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1229727459",
    "id" : "2BADDD1E",
    "lastKnownWriteTime" : 1490807522,
    "last_content_update" : 1490807522031,
    "path" : "~/WebstormProjects/negationMT/R_scripts/LDA.R",
    "project_path" : "R_scripts/LDA.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}